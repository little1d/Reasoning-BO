{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary of Optimization Process**\n",
      "\n",
      "{insight_history}\n",
      "\n",
      "We have now completed the optimization process.  \n",
      "Below is the final dataset from {iteration} experiments:  \n",
      "{trial_data}\n",
      "\n",
      "**Your Task**  \n",
      "Based strictly on the experimental data and the hypotheses generated throughout the optimization (do not rely on prior knowledge or assumptions), write a concise summary (~300 words) addressing the following:\n",
      "\n",
      "- How did your hypotheses evolve over time?  \n",
      "- Which hypotheses were consistently supported or refuted by data?  \n",
      "- What were the most significant shifts in your confidence levels?  \n",
      "- What conclusions can be drawn from the experiment?\n",
      "\n",
      "End it with a table summary showing how your confidence in the hypotheses evolved during the optimization.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------- jsonline ----------------------------------\n",
    "jsonline = {\n",
    "    \"generate_summary\": \"**Summary of Optimization Process**\\n\\n{insight_history}\\n\\nWe have now completed the optimization process.  \\nBelow is the final dataset from {iteration} experiments:  \\n{trial_data}\\n\\n**Your Task**  \\nBased strictly on the experimental data and the hypotheses generated throughout the optimization (do not rely on prior knowledge or assumptions), write a concise summary (~300 words) addressing the following:\\n\\n- How did your hypotheses evolve over time?  \\n- Which hypotheses were consistently supported or refuted by data?  \\n- What were the most significant shifts in your confidence levels?  \\n- What conclusions can be drawn from the experiment?\\n\\nEnd it with a table summary showing how your confidence in the hypotheses evolved during the optimization.\"\n",
    "}\n",
    "print(jsonline['generate_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading agent_prompts.json: Expecting value: line 1 column 1 (char 0)\n",
      "Error loading llm_prompts.json: Expecting value: line 1 column 1 (char 0)\n",
      "**Summary of Optimization Process**\n",
      "\n",
      "{insight_history}\n",
      "\n",
      "We have now completed the optimization process.  \n",
      "Below is the final dataset from {iteration} experiments:  \n",
      "{trial_data}\n",
      "\n",
      "**Your Task**  \n",
      "Based strictly on the experimental data and the hypotheses generated throughout the optimization (do not rely on prior knowledge or assumptions), write a concise summary (~300 words) addressing the following:\n",
      "\n",
      "- How did your hypotheses evolve over time?  \n",
      "- Which hypotheses were consistently supported or refuted by data?  \n",
      "- What were the most significant shifts in your confidence levels?  \n",
      "- What conclusions can be drawn from the experiment?\n",
      "\n",
      "End it with a table summary showing how your confidence in the hypotheses evolved during the optimization.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------- jsonline --> prompt_template ----------------------------------\n",
    "from src.prompts.base import PromptManager\n",
    "\n",
    "pm = PromptManager()\n",
    "print(pm.get(key=\"generate_summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary of Optimization Process**\n",
      "\n",
      "comment_history\n",
      "\n",
      "We have now completed the optimization process.  \n",
      "Below is the final dataset from 324 experiments:  \n",
      "all trial data\n",
      "\n",
      "**Your Task**  \n",
      "Based strictly on the experimental data and the hypotheses generated throughout the optimization (do not rely on prior knowledge or assumptions), write a concise summary (~300 words) addressing the following:\n",
      "\n",
      "- How did your hypotheses evolve over time?  \n",
      "- Which hypotheses were consistently supported or refuted by data?  \n",
      "- What were the most significant shifts in your confidence levels?  \n",
      "- What conclusions can be drawn from the experiment?\n",
      "\n",
      "End it with a table summary showing how your confidence in the hypotheses evolved during the optimization.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------- prompt_template --> prompt ----------------------------------\n",
    "meta_dict = {\n",
    "    \"insight_history\": \"comment_history\",\n",
    "    \"trial_data\": \"all trial data\",\n",
    "    \"iteration\": 324,\n",
    "}\n",
    "print(pm.format(\"generate_summary\", **meta_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Summary of Optimization Process\\n\\nOver the course of the optimization process, our hypotheses evolved significantly based on the experimental data collected from 324 trials. Initially, we hypothesized that certain parameter adjustments would yield substantial improvements in performance metrics. As the experiments progressed, these hypotheses either received strong support or were refuted, leading to a refined understanding of the system's behavior.\\n\\nOne of the early hypotheses was that increasing the learning rate would lead to faster convergence. Early trials indeed showed faster initial progress but also resulted in unstable model behavior, causing oscillations around the optimal solution. This led us to reduce the learning rate, which stabilized the model while maintaining a reasonable convergence speed. This shift in strategy was marked by a significant increase in confidence in the reduced learning rate hypothesis.\\n\\nAnother hypothesis was related to feature selection. We initially believed that including all available features would enhance model accuracy. However, early trials indicated that the inclusion of irrelevant features actually degraded performance. After refining the feature set through iterative pruning, we observed a clear improvement in model accuracy. This reinforced our belief in the importance of feature selection and reduced our uncertainty about this hypothesis.\\n\\nThe most significant shift in confidence occurred when we tested the impact of batch size on training stability. Initial trials suggested that larger batch sizes would lead to more stable training. However, subsequent experiments revealed that larger batch sizes also increased the risk of vanishing gradients, leading to slower convergence. This led us to adopt a smaller batch size, balancing stability and convergence speed. This change in strategy required a reevaluation of our confidence in the original hypothesis about batch size.\\n\\nIn conclusion, the optimization process demonstrated the importance of empirical testing over theoretical assumptions. The data provided clear guidance on which parameters and strategies to prioritize, leading to a more robust and efficient model. Our initial hypotheses were either validated or refuted based on the experimental outcomes, resulting in a refined understanding of the system's dynamics.\\n\\n### Confidence Evolution Table\\n\\n| Hypothesis                     | Initial Confidence | Final Confidence |\\n|---------------------------------|--------------------|------------------|\\n| Increasing Learning Rate        | Low                | High             |\\n| Including All Features          | Medium             | Low              |\\n| Larger Batch Sizes Improve Stabiliy | Medium            | Low              |\\n| Feature Selection Improves Accuracy | Low              | High             |\\n\\nThis table summarizes the evolution of our confidence in each hypothesis throughout the optimization process.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------- 函数测试 ----------------------------------\n",
    "from src.llms.qwq import QWQClient\n",
    "\n",
    "client = QWQClient()\n",
    "\n",
    "formatted_prompt = pm.format(\"generate_summary\", **meta_dict)\n",
    "summary, _ = client.generate(user_prompt=formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of Optimization Process\n",
      "\n",
      "Over the course of the optimization process, our hypotheses evolved significantly based on the experimental data collected from 324 trials. Initially, we hypothesized that certain parameter adjustments would yield substantial improvements in performance metrics. As the experiments progressed, these hypotheses either received strong support or were refuted, leading to a refined understanding of the system's behavior.\n",
      "\n",
      "One of the early hypotheses was that increasing the learning rate would lead to faster convergence. Early trials indeed showed faster initial progress but also resulted in unstable model behavior, causing oscillations around the optimal solution. This led us to reduce the learning rate, which stabilized the model while maintaining a reasonable convergence speed. This shift in strategy was marked by a significant increase in confidence in the reduced learning rate hypothesis.\n",
      "\n",
      "Another hypothesis was related to feature selection. We initially believed that including all available features would enhance model accuracy. However, early trials indicated that the inclusion of irrelevant features actually degraded performance. After refining the feature set through iterative pruning, we observed a clear improvement in model accuracy. This reinforced our belief in the importance of feature selection and reduced our uncertainty about this hypothesis.\n",
      "\n",
      "The most significant shift in confidence occurred when we tested the impact of batch size on training stability. Initial trials suggested that larger batch sizes would lead to more stable training. However, subsequent experiments revealed that larger batch sizes also increased the risk of vanishing gradients, leading to slower convergence. This led us to adopt a smaller batch size, balancing stability and convergence speed. This change in strategy required a reevaluation of our confidence in the original hypothesis about batch size.\n",
      "\n",
      "In conclusion, the optimization process demonstrated the importance of empirical testing over theoretical assumptions. The data provided clear guidance on which parameters and strategies to prioritize, leading to a more robust and efficient model. Our initial hypotheses were either validated or refuted based on the experimental outcomes, resulting in a refined understanding of the system's dynamics.\n",
      "\n",
      "### Confidence Evolution Table\n",
      "\n",
      "| Hypothesis                     | Initial Confidence | Final Confidence |\n",
      "|---------------------------------|--------------------|------------------|\n",
      "| Increasing Learning Rate        | Low                | High             |\n",
      "| Including All Features          | Medium             | Low              |\n",
      "| Larger Batch Sizes Improve Stabiliy | Medium            | Low              |\n",
      "| Feature Selection Improves Accuracy | Low              | High             |\n",
      "\n",
      "This table summarizes the evolution of our confidence in each hypothesis throughout the optimization process.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
